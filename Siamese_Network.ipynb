{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese Network",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqeU8/QtJL6G4cmJvDaVAA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyanshchordia/Faces/blob/master/Siamese_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDMURKIHPI7E",
        "colab_type": "text"
      },
      "source": [
        "# Siamese Neural Network\n",
        "\n",
        "Siamese neural network is a class of neural network architectures that contain two or more identical sub networks. identical here means they have the same configuration with the same parameters and weights. Parameter updating is mirrored across both sub networks.It is used find the similarity of the inputs by comparing its feature vectors.\n",
        "\n",
        "For more details check this blog : https://innovationincubator.com/siamaese-neural-network-with-paytorch-code-example/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLqOxtMiPgSJ",
        "colab_type": "text"
      },
      "source": [
        "## Steps to create classifier using Siamese Neural Network \n",
        "\n",
        "\n",
        "1.   **Data Preprocessing**\n",
        "2.   **Define the Siamese Network**\n",
        "3.   **Feature Vector Extraction**\n",
        "4.   **Similarity Score Calculation**\n",
        "5.   **Defininf Loss Function**\n",
        "6.   **Optimizer**\n",
        "7.   **Testing using One-Shot Learnig**\n",
        "8.   **Making Predictions**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQVRApGcQiKR",
        "colab_type": "text"
      },
      "source": [
        "Let's  go step wise "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esGrw0waQlwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import all the necessary Library \n",
        "import torchvision\n",
        "import torch.utils.data as utils\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils\n",
        "import numpy as np\n",
        "import time\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEJiWZVGIsot",
        "colab_type": "text"
      },
      "source": [
        "##Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnOghSpi9005",
        "colab_type": "code",
        "outputId": "34e634aa-af9b-4bd0-b646-b41aa268d924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from sklearn.datasets import fetch_lfw_pairs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pj-T2glebhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lfw_people = fetch_lfw_people(min_faces_per_person=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31QXS9c-cWTC",
        "colab_type": "code",
        "outputId": "a56bae1e-30ef-4614-e337-96be0df2da16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(lfw_people.data.shape,lfw_people.images.shape,lfw_people.target.shape,lfw_people.target_names.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4324, 2914) (4324, 62, 47) (4324,) (158,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DunURe4hVVOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [None]*lfw_people.target.shape[0]\n",
        "for i in range(len(data)):\n",
        "  data[i]=[]\n",
        "  data[i].append(lfw_people.target[i,])\n",
        "  data[i].append(lfw_people.target_names[data[i][0],])\n",
        "  data[i].append(lfw_people.data[i,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ7409YHYIj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_dict = {}\n",
        "for i in lfw_people.target_names:\n",
        "  image_dict[i]=[]\n",
        "\n",
        "for i in data:\n",
        "  image_dict[i[1]].append(i[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sTMOzRzdPLp",
        "colab_type": "code",
        "outputId": "f4de693c-9f0a-4fbb-cc1c-29972ab50454",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum=0\n",
        "for i in image_dict.values():\n",
        "  sum+=len(i)\n",
        "print(sum)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D1ESYvTkZFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYNElpCUfcW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = [None]*(len(data)*1)\n",
        "index = 0\n",
        "# each sample of the dataset must have an achor,positive and negative\n",
        "# I have 4324 images. If using each image I generate 1 sample, then I get a training data of size 4324\n",
        "# I have data and image_dict\n",
        "# data has all the images along with their label\n",
        "# image_dict has seperated images using label names\n",
        "for i in data:\n",
        "    dataset[index]=[]\n",
        "    dataset[index].append(i[2]) # 0\n",
        "    temp = random.randrange(len(image_dict[i[1]])) \n",
        "    dataset[index].append(image_dict[i[1]][temp]) # 1\n",
        "    temp = random.randrange(len(data))\n",
        "    if(data[temp][0]==i[0]) : temp = (temp+1)%len(data)\n",
        "    dataset[index].append(data[temp][2]) # 2\n",
        "    dataset[index].append([i[1],i[1],data[temp][1]]) # label\n",
        "    index+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODHaxpugtvu3",
        "colab_type": "code",
        "outputId": "41476d39-7b6a-4a8a-836a-7d6b04691e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "print(len(dataset))\n",
        "print(dataset[0],dataset[10],dataset[1000],sep=\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4324\n",
            "[array([95.333336, 80.666664, 95.      , ..., 35.      , 39.      ,\n",
            "       40.      ], dtype=float32), array([95.333336, 80.666664, 95.      , ..., 35.      , 39.      ,\n",
            "       40.      ], dtype=float32), array([ 94.666664,  93.666664, 100.      , ...,  63.666668,  58.      ,\n",
            "        55.333332], dtype=float32), ['Jacques Chirac', 'Jacques Chirac', 'Hillary Clinton']]\n",
            "[array([ 10.       ,   7.3333335,   5.       , ..., 159.33333  ,\n",
            "       161.33333  , 158.33333  ], dtype=float32), array([100.333336,  90.666664,  66.666664, ...,  84.      ,  75.      ,\n",
            "        58.333332], dtype=float32), array([ 47.      ,  59.666668,  74.      , ..., 225.33333 , 224.66667 ,\n",
            "       223.66667 ], dtype=float32), ['Gloria Macapagal Arroyo', 'Gloria Macapagal Arroyo', 'Jean-David Levitte']]\n",
            "[array([ 10.666667,  11.333333,  45.      , ..., 183.33333 , 185.      ,\n",
            "       189.66667 ], dtype=float32), array([145.      , 141.33333 , 144.      , ..., 143.66667 ,  52.      ,\n",
            "        12.333333], dtype=float32), array([34.666668, 32.666668, 27.      , ..., 16.666666, 13.      ,\n",
            "       11.      ], dtype=float32), ['Colin Powell', 'Colin Powell', 'Jackie Chan']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4LisaWBuKRn",
        "colab_type": "text"
      },
      "source": [
        "Our dataset is ready with 4324 samples to train our siamese model over"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icbREhBouWCe",
        "colab_type": "code",
        "outputId": "3f1a8946-a55f-4578-e1ef-49ee63ba9bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = np.array(dataset)\n",
        "dataset = dataset[:,0:3]\n",
        "print(dataset.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4324, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBOpcs9KRjWm",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing and Loading Dataset\n",
        "\n",
        "We preprocessed all the images and loaded them as .npy files which is easy to transfer . You can follow your own preprocessing steps .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC5JhgpcPmP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetworkDataset():\n",
        "    \n",
        "    def __init__(self,data=None,transform=None):\n",
        "        # used to prepare the labels and images path\n",
        "        self.data = data \n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        \n",
        "        img0 = torch.tensor(self.data[index][0].reshape(62,47))\n",
        "        img1 = torch.tensor(self.data[index][1].reshape(62,47))\n",
        "        img2 = torch.tensor(self.data[index][2].reshape(62,47))\n",
        "        # Apply image transformations\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "        \n",
        "        return img0,img1,img2\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws1NipMUQaI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the the dataset from raw image folders\n",
        "siamese_dataset = SiameseNetworkDataset(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTpprbksZS80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Viewing the sample of images and to check whether its loading properly\n",
        "vis_dataloader = DataLoader(siamese_dataset,\n",
        "                        shuffle=True,\n",
        "                        batch_size=8)\n",
        "dataiter = iter(vis_dataloader)\n",
        "\n",
        "\n",
        "example_batch = next(dataiter)\n",
        "print(example_batch[0].size())\n",
        "print(example_batch[1].size())\n",
        "print(example_batch[2].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xhlmF-nj6_v",
        "colab_type": "text"
      },
      "source": [
        "## Siamese Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjkBkyTomlU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        \n",
        "        # Setting up the Sequential of CNN Layers\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, kernel_size=11,stride=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            \n",
        "            nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.Dropout2d(p=0.3),\n",
        "\n",
        "            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            nn.Dropout2d(p=0.3),\n",
        "\n",
        "        )\n",
        "        \n",
        "        # Defining the fully connected layers\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(3840, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(p=0.5),\n",
        "            \n",
        "            nn.Linear(1024, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Linear(128,2))\n",
        "        \n",
        "  \n",
        "  \n",
        "    def forward_once(self, x):\n",
        "        # Forward pass \n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input1, input2,input3):\n",
        "        # forward pass of input 1\n",
        "        output1 = self.forward_once(input1)\n",
        "        # forward pass of input 2\n",
        "        output2 = self.forward_once(input2)\n",
        "        # forward pass of input 3\n",
        "        output3 = self.forward_once(input3)\n",
        "        return output1, output2, output3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xaKRzCV-vBD",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBjCIlhWk2MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TripleLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(TripleLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor , positive , negative):\n",
        "        d_ap = F.pairwise_distance(anchor, positive)\n",
        "        d_an = F.pairwise_distance(anchor, negative)\n",
        "        x = (d_ap - d_an + self.margin).float().mean()\n",
        "        if x>0:\n",
        "          triple_loss = x\n",
        "        else: triple_loss = x - x\n",
        "\n",
        "        return triple_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyGA_GUt-0xp",
        "colab_type": "text"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO9uIznkXiB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the dataset as pytorch tensors using dataloader\n",
        "train_dataloader = DataLoader(siamese_dataset,\n",
        "                        shuffle=True,\n",
        "                        num_workers=8,\n",
        "                        batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tspmR_2bd824",
        "colab_type": "code",
        "outputId": "1600bceb-7308-43e7-e68c-d5eccd26d04c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Check whether you have GPU is loaded or not\n",
        "if torch.cuda.is_available():\n",
        "    print('Yes')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnzoTXfE5cX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declare Siamese Network\n",
        "net = SiameseNetwork().cuda()\n",
        "# Decalre Loss Function\n",
        "criterion = TripleLoss()\n",
        "# Declare Optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(),lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aUJOhkrFfu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    counter = []\n",
        "    loss_history = [] \n",
        "    iteration_number= 0\n",
        "    \n",
        "    for epoch in range(0,epochs):\n",
        "        temp=[]\n",
        "        for i, data in enumerate(train_dataloader,0):\n",
        "            img0, img1 , img2 = data\n",
        "            img0, img1 , img2 = img0.cuda(), img1.cuda() , img2.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            img0 = img0.reshape(-1,1,62,47)\n",
        "            img1 = img1.reshape(-1,1,62,47)\n",
        "            img2 = img2.reshape(-1,1,62,47)\n",
        "            output1,output2,output3 = net(img0,img1,img2)\n",
        "            loss = criterion(output1,output2,output3)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            temp.append(loss.item())\n",
        "        loss_history.append(np.asarray(temp).mean())\n",
        "        print(loss_history[-1])\n",
        "    \n",
        "    plt.plot(loss_history)\n",
        "    plt.show()\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDx471-mBDGD",
        "colab_type": "code",
        "outputId": "7aa1a72d-b320-43a4-944e-51cbdd9c9358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "epochs= 10\n",
        "batchsize=256\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Train the model\n",
        "model = train()\n",
        "torch.save(model.state_dict(), \"/content/model.pt\")\n",
        "print(\"Model Saved Successfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.257913796340718\n",
            "2.1230578001807716\n",
            "2.699061393737793\n",
            "1.7247612897087545\n",
            "1.5363181969698738\n",
            "0.67587490642772\n",
            "0.18471094440011418\n",
            "0.20510726816513958\n",
            "0.4904266806209789\n",
            "0.9190531969070435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model Saved Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YcvO7iVof7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EH_2bML3v2W",
        "colab_type": "code",
        "outputId": "cc257d22-9729-4593-83c8-3f87bf1e5fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the saved model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SiameseNetwork()\n",
        "model.load_state_dict(torch.load(\"/content/model.pt\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo5Jl-txHNHX",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwwAtc6sTliR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_dist(a1,a2):\n",
        "  return F.pairwise_distance(a1,a2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZHnXdNFS0Fy",
        "colab_type": "code",
        "outputId": "73043c6d-8a11-4b41-9a3d-dbcca0a24871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "sim_arr=[]\n",
        "dissim_arr=[]\n",
        "for i in dataset[0:100,:]:\n",
        "  with torch.no_grad():\n",
        "    img0 = torch.tensor(i[0].reshape(-1,1,62,47))\n",
        "    img1 = torch.tensor(i[1].reshape(-1,1,62,47))\n",
        "    img2 = torch.tensor(i[2].reshape(-1,1,62,47))\n",
        "    a,p,n = model(img0,img1,img2)\n",
        "    sim_arr.append(cal_dist(a,p))\n",
        "    dissim_arr.append(cal_dist(a,n))\n",
        "s = np.asarray(sim_arr).mean()\n",
        "d = np.asarray(dissim_arr).mean()\n",
        "print(\"Mean distance between the embeddings of Similar images is\",s)\n",
        "print(\"Mean distance between the embeddings of Dissimilar images is\",d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean distance between the embeddings of Similar images is 77.63261\n",
            "Mean distance between the embeddings of Dissimilar images is 96.963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzU46xtYh3L8",
        "colab_type": "text"
      },
      "source": [
        "So even on just 5 epochs, my model is taking a large amount of time.\n",
        "\n",
        "My focus is not on building a perfectly trained Siamese Network for one shot learning, rather my focus is on understanding that how exactly a siamese network is able to do this.\n",
        "\n",
        "The triple loss focuses on increasing the distance between the embeddings of dissimliar image in every step. If we have enough time and computation power then training this model with even better samples and for larger number of epochs  we can build a very strong siamese network that can produce perfect embeddings for face recognition.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOrN15LCp7jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}